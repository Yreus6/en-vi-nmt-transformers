{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-15T00:58:37.174271100Z",
     "start_time": "2023-10-15T00:58:35.894590300Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.transformers.seq2seq_trans import seq2seq_trans\n",
    "from models.transformers.utils import DEVICE, translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class obj(object):\n",
    "    def __init__(self, d):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(k, (list, tuple)):\n",
    "                setattr(self, k, [obj(x) if isinstance(x, dict) else x for x in v])\n",
    "            else:\n",
    "                setattr(self, k, obj(v) if isinstance(v, dict) else v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T00:58:37.182273Z",
     "start_time": "2023-10-15T00:58:37.177271900Z"
    }
   },
   "id": "77e667ab4566db80"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hieu/.local/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": "Seq2SeqTransformer(\n  (transformer): Transformer(\n    (encoder): TransformerEncoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerEncoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): TransformerDecoder(\n      (layers): ModuleList(\n        (0-2): 3 x TransformerDecoderLayer(\n          (self_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (multihead_attn): MultiheadAttention(\n            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n          )\n          (linear1): Linear(in_features=512, out_features=512, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (linear2): Linear(in_features=512, out_features=512, bias=True)\n          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (dropout1): Dropout(p=0.1, inplace=False)\n          (dropout2): Dropout(p=0.1, inplace=False)\n          (dropout3): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (generator): Linear(in_features=512, out_features=52000, bias=True)\n  (src_tok_emb): TokenEmbedding(\n    (embedding): Embedding(52000, 512)\n  )\n  (tgt_tok_emb): TokenEmbedding(\n    (embedding): Embedding(52000, 512)\n  )\n  (positional_encoding): PositionalEncoding(\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = obj({\n",
    "    'src_vocab_path': './vocab/vocab.en.json',\n",
    "    'tgt_vocab_path': './vocab/vocab.vi.json',\n",
    "    'model_file': './checkpoints/1015-023833/best_model.pth'\n",
    "})\n",
    "\n",
    "model = seq2seq_trans(args.model_file)\n",
    "model.eval()\n",
    "model.to(DEVICE)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T00:58:39.322014Z",
     "start_time": "2023-10-15T00:58:37.181271300Z"
    }
   },
   "id": "5fdddd013e7868ac"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ngày nay tôi phải đảm bảo dự án của tôi']\n"
     ]
    }
   ],
   "source": [
    "with torch.set_grad_enabled(False):\n",
    "    outputs = translate(model, ['Today I must defend my project'], args)\n",
    "print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T00:58:51.798523800Z",
     "start_time": "2023-10-15T00:58:51.649220200Z"
    }
   },
   "id": "dc7aaed0460e5c8d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T00:58:40.238166200Z",
     "start_time": "2023-10-15T00:58:40.234166200Z"
    }
   },
   "id": "4697d5ae32cca58e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
